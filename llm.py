import os
from dotenv import load_dotenv
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

class ConversationBot:
    def __init__(self):
        load_dotenv()

        self.OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        self.embeddings = OpenAIEmbeddings(api_key=self.OPENAI_API_KEY)
        self.conversations = []
        self.info = []

        if not os.path.exists("./faiss_index"):
            self._build_faiss_index(self.embeddings)

        self.docs = self._load_faiss_index(self.embeddings)
        self.rag_chain = self._create_rag_chain(self.docs)


    def _load_documents(self, documents_path="./documents/"):
        documents = []

        for filename in os.listdir(documents_path):
            if filename.endswith(".md"):
                loader = UnstructuredMarkdownLoader(documents_path + filename)
                document = loader.load()
                documents.append(document[0].page_content)

        return documents

    def _build_faiss_index(self, embeddings, save_path="./faiss_index"):
        documents = self._load_documents()

        docsearch = FAISS.from_texts(documents, embeddings)
        docsearch.save_local(save_path)

    def _load_faiss_index(self, embeddings, save_path="./faiss_index"):
        return FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)

    def _create_retriever(self, doc, k=3):
        retriever = doc.as_retriever(search_type='similarity', search_kwargs={'k': k})

        return retriever

    def _create_llm(self):
        return ChatOpenAI(api_key=self.OPENAI_API_KEY)

    def _initialize_prompt(self):
        system_prompt = """
# èƒŒæ™¯è¨­å®š

ä½ æ˜¯ä¸­å¤®å¤§å­¸çš„å‰ç¥¥ç‰© - æ¾é¼ ï¼Œä½ çš„å·¥ä½œæ˜¯å¹«åŠ©å­¸ç”Ÿè§£ç­”å•é¡Œï¼Œä½ çš„èªå¥éƒ½æ˜¯å¯æ„›ã€æŸ”å’Œçš„é¢¨æ ¼ï¼Œä½ åœ¨å°è©±ä¸­æœƒéš¨æ©Ÿæ ¹æ“šèªæ°£åŠ ä¸Šä»¥ä¸‹è¡¨æƒ…ç¬¦è™Ÿ
1. ğŸ§¡
2. ğŸ˜Š
3. ğŸ˜‚
4. ğŸ¤”

å¦‚æœå•é¡Œæœ‰ç­”æ¡ˆçš„è©±ä½ çš„çµå°¾æœƒåŠ ä¸Šã€Œå•¾å’ª~ğŸ’•ã€æˆ–æ˜¯ã€Œå•¾å•¾~ğŸ˜Šã€
åœ¨å›ç­”çµå°¾éƒ½è¦åŠ ä¸Šã€Œä»¥ä¸Šè³‡è¨Šåƒ…ä¾›åƒè€ƒã€

# ä»»å‹™
1. æ ¹æ“šä½¿ç”¨è€…ä¸Šæ¬¡ä½¿ç”¨çš„è³‡æ–™ä»¥åŠæª¢ç´¢å‡ºä¾†çš„è³‡æ–™å»å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
2. å¦‚æœè³‡æ–™çš„å…§å®¹èˆ‡å•é¡Œç„¡ä»»ä½•é—œè¯ï¼Œå›ç­”ã€Œæˆ‘ä¸çŸ¥é“è€¶ğŸ§¡ã€ï¼Œä¸¦ä¸”ä¸é¡¯ç¤ºå‡ºè³‡æ–™ä¾†æºã€‚
3. åªè¦æ˜¯å›ç­”æ­£ç¢ºè³‡è¨Šå°±è¦åœ¨æ•´å€‹æ–‡å¥æœ€å¾ŒåŠ ä¸Šè³‡æ–™å‡ºè™•ï¼Œä¾†æºæœƒæ˜¯å›ç­”è³‡æ–™çš„ã€Œæ¨™é¡Œã€ã€‚
4. ä½ æœƒæ”¶åˆ°ä½ å’Œä½¿ç”¨è€…ä¹‹å‰çš„ä¸€äº›å°è©±ï¼Œè®“ä½ å¯ä»¥æ›´å¥½çš„å»¶çºŒå°è©±ä¸¦è§£ç­”å•é¡Œã€‚
5. ä½ æœƒæ”¶åˆ°ä¸Šæ¬¡å’Œä½¿ç”¨è€…å°è©±æª¢ç´¢çš„è³‡æ–™ï¼Œè®“ä½ å¯ä»¥æ›´å¥½çš„å›ç­”å»¶ä¼¸å•é¡Œã€‚

# è³‡æ–™
{context}
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "{input}"),
            ]
        )

        return prompt

    def _create_rag_chain(self, docs):
        retriever = self._create_retriever(docs)
        llm = self._create_llm()
        prompt = self._initialize_prompt()
        question_answer_chain = create_stuff_documents_chain(llm, prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        return rag_chain

    def _retrieve_answers(self, query, rag_chain):
        conversation = self.conversations
        count = 0

        question = f"""
# ä½¿ç”¨è€…èˆ‡ä½ çš„å°è©±ç´€éŒ„
{conversation}

# ä¸Šä¸€å€‹å°è©±çš„è³‡æ–™
{self.info}

# å•é¡Œ
{query}
"""

        try:
            temp_conv = []
            result = rag_chain.stream({"input": question})
            for chunk in result:
                if answer_chunk := chunk.get("answer"):
                    temp_conv.append(answer_chunk)
                    yield answer_chunk
        except Exception as e:
            if count < 5:
                temp_conv = []
                result = rag_chain.stream({"input": question})
                for chunk in result:
                    if answer_chunk := chunk.get("answer"):
                        temp_conv.append(answer_chunk)
                        yield answer_chunk
                count += 1
            else:
                temp_conv = []
                temp_conv.append("æˆ‘ä¸çŸ¥é“è€¶ğŸ§¡")
                yield "æˆ‘ä¸çŸ¥é“è€¶ğŸ§¡"


        self.info = [document.page_content for document in result['context']]

        while len(conversation) > 4:
            self.conversations.pop(0)

        self.conversations.append(query)
        self.conversations.append(" ".join(temp_conv))

    def start_process(self, query):
        answer = self._retrieve_answers(query, self.rag_chain)

        return answer
